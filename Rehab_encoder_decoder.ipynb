{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9cdd3b",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install torch\n",
    "# !{sys.executable} -m pip install tesnorflow\n",
    "# !{sys.executable} -m pip install torchvision\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install pytorch-lightning\n",
    "# !{sys.executable} -m pip install lightning-bolts\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "# !{sys.executable} -m pip install mne-features\n",
    "# !{sys.executable} -m pip install fitter\n",
    "# !{sys.executable} -m pip install lightgbm\n",
    "# !{sys.executable} -m pip install import_ipynb\n",
    "# !{sys.executable} -m pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d8214",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io\n",
    "import mne\n",
    "import sklearn\n",
    "import os \n",
    "import random\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from itertools import chain, product\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mne_features.feature_extraction import FeatureExtractor\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from scipy.stats import norm, wasserstein_distance\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ca5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Utility functions frmo diffrent notebooks\n",
    "import import_ipynb\n",
    "from IEEE_data import extract_ieee_data, LazyProperty, data_4class\n",
    "from CHIST_ERA_data import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b51bd",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57663551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eegFilters(eegMat, fs, filterLim):\n",
    "    eegMatFiltered = mne.filter.filter_data(eegMat, fs, filterLim[0], filterLim[1], verbose=0)\n",
    "    return eegMatFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mneFeatures(X, fs):\n",
    "    selected_funcs = ['line_length', 'kurtosis', 'skewness', 'pow_freq_bands', 'spect_slope',\n",
    "                     'spect_entropy', 'spect_edge_freq', 'mean', 'variance', 'ptp_amp']\n",
    "    params = {'pow_freq_bands__freq_bands' : np.array([[8, 10],\n",
    "                                                      [10,12],\n",
    "                                                      [9, 13],\n",
    "                                                      [12, 20],\n",
    "                                                      [20, 25],\n",
    "                                                      [25, 30]])}\n",
    "    fe = FeatureExtractor(sfreq=fs, selected_funcs=selected_funcs, params=params)\n",
    "\n",
    "    X_features = fe.fit_transform(X)\n",
    "    return X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadefa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2csp(signal, labels, n_components):\n",
    "    \n",
    "    # Set verbose to 0\n",
    "    mne.set_log_level(verbose='WARNING', return_old_level=False, add_frames=None)\n",
    "\n",
    "    # create csp object\n",
    "    csp = mne.decoding.CSP(n_components=n_components, reg=None, log=None, norm_trace=False, transform_into='csp_space')\n",
    "    # transofrm the signal\n",
    "    csp.fit(signal, labels)\n",
    "    csp_signal = csp.transform(signal)\n",
    "    return csp_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSignal(elec, model, dataLoader):\n",
    "    # Create iterable object\n",
    "    data_iter = iter(dataLoader)\n",
    "    data, _ = data_iter.next()\n",
    "\n",
    "    \n",
    "    # Reconstruct data using given model\n",
    "    recon_data = model(data).detach()\n",
    "    \n",
    "    \n",
    "    # Plot original and reconstructed data\n",
    "    plt.figure(1)\n",
    "    plt.plot(data[0, elec, :], zorder=1)\n",
    "    plt.plot(recon_data[0, elec, :], zorder=0)\n",
    "    plt.legend(['Original', 'Reconstructed'])\n",
    "    plt.title(f'mse-Loss: {F.mse_loss(recon_data[0, elec, :], data[0, elec, :])}')\n",
    "    plt.xlabel('Time [mS]')\n",
    "    plt.ylabel('μV')\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(data[0, elec, :] - recon_data[0, elec, :])\n",
    "    plt.legend(['Residulas'])\n",
    "    plt.title(f'Residuals Signal')\n",
    "    plt.xlabel('Time [mS]')\n",
    "    plt.ylabel('μV')\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d748b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_error(W, x):\n",
    "    x = torch.reshape(x, (x.shape[1], x.shape[0] * x.shape[2])).T\n",
    "    pc = torch.matmul(x, W)\n",
    "    x_rec = torch.matmul(pc, W.T)\n",
    "    error = torch.norm(x - x_rec)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcd17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csp_score(signal, labels, cv_N = 5, classifier = False):\n",
    "    \n",
    "    # Set verbose to 0\n",
    "    mne.set_log_level(verbose='WARNING', return_old_level=False, add_frames=None)\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    \n",
    "    if classifier:\n",
    "        y_pred = classifier.predict(signal)\n",
    "        acc = sklearn.metrics.accuracy_score(labels, y_pred)\n",
    "        return acc\n",
    "    \n",
    "    else:\n",
    "        # Assemble a classifier\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "#         lda = sklearn.ensemble.RandomForestClassifier()\n",
    "        csp = mne.decoding.CSP(n_components=26, reg=None, log=False, norm_trace=True)\n",
    "        # Use scikit-learn Pipeline with cross_val_score function\n",
    "        clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "        scores = cross_val_score(clf, signal, labels, cv=cv_N, n_jobs=1)\n",
    "        clf.fit(signal, labels)\n",
    "        return np.mean(scores), clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56701998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mne_classifier(signal, labels, fs, zero_time, classifier = False):\n",
    "    # cut signal before + after\n",
    "    signal_before = signal[:, :, :fs*zero_time]\n",
    "    signal_after = signal[:, :, fs*zero_time:]\n",
    "    # Get features\n",
    "    features_before = mneFeatures(signal_before, fs)\n",
    "    features_after = mneFeatures(signal_after, fs)\n",
    "    \n",
    "#     X = np.hstack((features_before, features_after))\n",
    "    X = np.divide(features_before, features_after)\n",
    "    if classifier:\n",
    "        score = features_classfier(X, labels, 5, classifier)\n",
    "        return score\n",
    "    else:\n",
    "        score, clf = features_classfier(X, labels, 5, classifier)\n",
    "    return score, clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_classfier(X, y, cv_N = 5, classifier = False):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    if classifier:\n",
    "        y_pred = classifier.predict(X)\n",
    "        acc = sklearn.metrics.accuracy_score(y, y_pred)\n",
    "        return acc\n",
    "    \n",
    "    else:\n",
    "        # Assemble a classifier\n",
    "#         clf = RandomForestClassifier()\n",
    "        clf = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n",
    "        scores = cross_val_score(clf, X, y, cv=cv_N, n_jobs=1)\n",
    "        clf.fit(X, y)\n",
    "        return np.mean(scores), clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_day_classifier(eeg_list):\n",
    "    # Use day zero classifier for classifying the reconstructed eeg per day\n",
    "    \n",
    "    residuals = []\n",
    "    labels = []\n",
    "    for day_i in range(len(eeg_list)):\n",
    "        # Get data\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, [day_i, day_i+1])\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "       \n",
    "        # Get residuals\n",
    "        residuals.append(signal_test.detach().numpy())\n",
    "        labels.append(np.ones((1, signal_test.shape[0])) * day_i)\n",
    "    labels= np.hstack(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    residuals = np.vstack(residuals)\n",
    "\n",
    "    score, _ = csp_score(np.float64(residuals), labels, cv_N = 5, classifier = False)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_day_classifier(AE_model, eeg_list):\n",
    "    # Use day zero classifier for classifying the reconstructed eeg per day\n",
    "    \n",
    "    residuals = []\n",
    "    labels = []\n",
    "    for day_i in range(len(eeg_list)):\n",
    "        # Get data\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, [day_i, day_i+1])\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "\n",
    "        # reconstruct EEG AE\n",
    "        rec_signal = AE_model(signal_test).detach().numpy()        \n",
    "        # Get residuals\n",
    "        residuals.append(rec_signal)\n",
    "        labels.append(np.ones((1, signal_test.shape[0])) * day_i)\n",
    "    labels= np.hstack(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    residuals = np.vstack(residuals)\n",
    "\n",
    "    score, _ = csp_score(np.float64(residuals), labels, cv_N = 5, classifier = False)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_day_classifier(AE_model, eeg_list):\n",
    "    # Use day zero classifier for classifying the residuals per day\n",
    "    \n",
    "    residuals = []\n",
    "    labels = []\n",
    "    for day_i in range(len(eeg_list)):\n",
    "        # Get data\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, [day_i, day_i+1])\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "\n",
    "        # reconstruct EEG AE\n",
    "        rec_signal = AE_model(signal_test).detach().numpy()        \n",
    "        # Get residuals\n",
    "        residuals.append((signal_test - rec_signal).detach().numpy())\n",
    "        labels.append(np.ones((1, signal_test.shape[0])) * day_i)\n",
    "    labels= np.hstack(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    residuals = np.vstack(residuals)\n",
    "\n",
    "    score, _ = csp_score(np.float64(residuals), labels, cv_N = 5, classifier = False)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77144ecd",
   "metadata": {},
   "source": [
    "### Datset and Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecefb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataSet_signal(Dataset):\n",
    "    def __init__(self, EEGDict, days_range=[0,1]):\n",
    "        \n",
    "        # Concat dict      \n",
    "        X, y = self.concat(EEGDict, days_range)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Convert from numpy to tensor\n",
    "        self.X = torch.tensor(X)\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.n_channels = self.X.shape[1]\n",
    "        self.y = y\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getAllItems(self):\n",
    "        return self.X.float() , self.y\n",
    "    \n",
    "    def concat(self, EEGDict, days_range):\n",
    "        X = []\n",
    "        y = []\n",
    "        for d in dictListStacked[days_range[0]:days_range[1]]:\n",
    "            X.append(d['segmentedEEG'])\n",
    "            y.append(d['labels'])\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_AE(LightningModule):\n",
    "    def __init__(self, input_channels, learning_rate=1e-3, filters_n = [16, 32, 64], pca_W = False):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.filters_n = filters_n\n",
    "        self.learning_rate = learning_rate\n",
    "        self.float()\n",
    "        self.l1_filters, self.l2_filters, self.l3_filters = self.filters_n\n",
    "\n",
    "        self.pca_W = pca_W\n",
    "        ### The model architecture ###\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "        nn.Conv1d(self.input_channels, self.l1_filters, kernel_size=15, stride=2, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l1_filters, self.l2_filters, kernel_size=10, stride=2, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l2_filters, self.l3_filters, kernel_size=5, stride=2, padding=1),\n",
    "        nn.LeakyReLU()\n",
    "        )\n",
    "                \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "        # IMPORTENT - on the IEEE dataset - the output padding needs to be 1 in the row below -on CHIST-ERA its 1\n",
    "        nn.ConvTranspose1d(self.l3_filters, self.l2_filters, kernel_size=5, stride=2, padding=1, output_padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.ConvTranspose1d(self.l2_filters, self.l1_filters, kernel_size=10, stride=2, padding=1, output_padding=0),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.ConvTranspose1d(self.l1_filters, self.input_channels, kernel_size=15, stride=2, padding=1, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # Recountruction\n",
    "        logits = self.forward(x)\n",
    "        # Loss function\n",
    "        try:\n",
    "            if len(self.pca_W.shape)>0:\n",
    "                rec_error = pca_error(self.pca_W, logits)\n",
    "                loss = F.mse_loss(logits, x) + 0*rec_error\n",
    "        except:\n",
    "            loss = F.mse_loss(logits, x)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa2af9",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "channels names:\n",
    "['FC3', 'C1', 'C3', 'C5', 'CP3', 'O1', 'FC4', 'C2', 'C4', 'C6', 'CP4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205cc58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subID = '205' # As str 201, 205, 206\n",
    "eyesFlag = 'CC' # str        CC --> closed,   OO --> open\n",
    "dataDir = 'data'\n",
    "\n",
    "# To get all The days in subject 201:\n",
    "dayNumber = get_all_days(dataDir, subID, eyesFlag) # Array of the desired days number\n",
    "dayNumber.sort()\n",
    "# For subject 205 & 206 its better to insert range\n",
    "dayNumber = range(1,9)\n",
    "\n",
    "# Subject 201 has only 1 block\n",
    "block = [1,2,3]\n",
    "trialLen = 6 # In seconds\n",
    "filterLim = [8,30] # In Hz\n",
    "elec_idxs = range(11) # 0-10 according to channel names\n",
    "\n",
    "\n",
    "\n",
    "ae_learning_rt = 1e-3\n",
    "n_epochs = 50\n",
    "batch_sz = 16\n",
    "# If you want to use comparison rate - set layers_sz = False\n",
    "convolution_filters = [8,16,32] # Length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c76538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relative path to absolute path\n",
    "dataDir = os.path.abspath(dataDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42c491",
   "metadata": {},
   "source": [
    "### Load the files - CHIST ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all relevant days files into list\n",
    "dataList = getRecording(dataDir, subID, eyesFlag, dayNumber, block)\n",
    "\n",
    "# Extract and segment all the data\n",
    "dictList = []\n",
    "for dayData in dataList:\n",
    "    # Extract each day data\n",
    "    interData = extractData(dayData)\n",
    "    \n",
    "    # This condition is to remove some corrupted files in subject 201\n",
    "    if interData['EEG'].dtype != np.dtype('float64'):\n",
    "        continue\n",
    "        \n",
    "    # Filter the data\n",
    "    interData['EEG'] = eegFilters(interData['EEG'], interData['fs'], filterLim)\n",
    "    interData['EEG'] = interData['EEG'][elec_idxs, :]\n",
    "\n",
    "    # Segment the data\n",
    "    dictList.append(segmentEEG(interData, trialLen, printFlag=0))\n",
    "\n",
    "# Stack block of same day\n",
    "dictListStacked = stackBlocks(dictList, len(block))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b12566",
   "metadata": {},
   "source": [
    "### Load the files - IEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd30ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 'S6'\n",
    "tmin = -2\n",
    "tmax = 2\n",
    "select_label = [1,4]\n",
    "zero_time = 2\n",
    "fs=500\n",
    "\n",
    "\n",
    "dictListStacked = extract_ieee_data(sub, filterLim, tmin, tmax, select_label, data_dir = 'data/ieee_dataset/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc91d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_thresh = 150\n",
    "min_trials = 20\n",
    "\n",
    "# Remove noisy trials using amplitude threshold\n",
    "new_dict_list = []\n",
    "for i, D in enumerate(dictListStacked):\n",
    "    max_amp = np.amax(np.amax(D['segmentedEEG'], 2), 1)\n",
    "    min_amp = np.amin(np.amin(D['segmentedEEG'], 2), 1)\n",
    "    max_tr = max_amp > amp_thresh \n",
    "    min_tr = min_amp < -amp_thresh\n",
    "    noisy_trials = [a or b for a, b in zip(max_tr, min_tr)]\n",
    "    D['segmentedEEG'] = np.delete(D['segmentedEEG'], noisy_trials,axis=0)\n",
    "    D['labels'] = np.delete(D['labels'], noisy_trials,axis=0)\n",
    "    print('Day #',i, ' trials: ', D['segmentedEEG'].shape[0])\n",
    "    if D['segmentedEEG'].shape[0] > min_trials:\n",
    "            new_dict_list.append(D)\n",
    "    \n",
    "dictListStacked = new_dict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d26a62",
   "metadata": {},
   "source": [
    "# Training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55016a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_days, epoch_N, dictListStacked, ae_learning_rt, convolution_filters, batch_sz):\n",
    "    \n",
    "    # Logger\n",
    "    logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "\n",
    "    # Train Dataset\n",
    "    signal_data = EEGDataSet_signal(dictListStacked, train_days)\n",
    "    signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "    x, y = signal_data.getAllItems()\n",
    "\n",
    "    # Train model on training day\n",
    "    day_zero_AE = convolution_AE(signal_data.n_channels, ae_learning_rt, filters_n=convolution_filters)\n",
    "    trainer_1 = pl.Trainer(max_epochs=epoch_N, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "    trainer_1.fit(day_zero_AE, train_dataloaders=signal_data_loader)\n",
    "    \n",
    "    # Day 0 classifier\n",
    "#     score_ae, day_zero_AE_clf = mne_classifier(day_zero_AE(x).detach().numpy(), y, fs, zero_time, classifier = False)\n",
    "#     score_bench, day_zero_bench_clf = mne_classifier(x.detach().numpy(), y, fs, zero_time, classifier = False)\n",
    "    score_ae, day_zero_AE_clf = csp_score(np.float64(day_zero_AE(x).detach().numpy()), y, cv_N = 5, classifier = False)\n",
    "    score_bench, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N = 5, classifier = False)\n",
    "\n",
    "    # Loop :)\n",
    "    bench_diff_day_score = []\n",
    "    bench_same_day_score = []\n",
    "    AE_diff_day_score = []\n",
    "    \n",
    "    # Append day zero score\n",
    "    bench_diff_day_score.append(score_bench)\n",
    "    bench_same_day_score.append(score_bench)\n",
    "    AE_diff_day_score.append(score_ae)\n",
    "\n",
    "    for i in range(train_days[1], len(dictListStacked)):\n",
    "        test_days = [i, i+1]\n",
    "\n",
    "        # Create test Datasets\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "        signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "        # get data\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "        # reconstruct EEG using day 0 AE\n",
    "        rec_signal_zero = day_zero_AE(signal_test).detach().numpy()\n",
    "\n",
    "\n",
    "        # Use models\n",
    "        print('Day #',i)\n",
    "        same_day_score, _ = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = False)\n",
    "#         same_day_score, _ = mne_classifier(signal_test.detach().numpy(), y_test, fs, zero_time, classifier = False)\n",
    "        print('Bench-\\nIn day accuracy: ', same_day_score)\n",
    "        bench_diff_day = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = day_zero_bench_clf)\n",
    "#         bench_diff_day  = mne_classifier(signal_test.detach().numpy(), y_test, fs, zero_time, day_zero_bench_clf)\n",
    "        print('Different day accuracy: ',bench_diff_day)\n",
    "\n",
    "        AE_diff_day = csp_score(rec_signal_zero, y_test, cv_N = 5, classifier = day_zero_AE_clf)\n",
    "#         AE_diff_day =  mne_classifier(rec_signal_zero, y_test, fs, zero_time, day_zero_AE_clf)\n",
    "        print('AE-\\nDifferent day accuracy: ', AE_diff_day, '\\n')\n",
    "\n",
    "        # Append each day results\n",
    "        AE_diff_day_score.append(AE_diff_day)\n",
    "        bench_diff_day_score.append(bench_diff_day) \n",
    "        bench_same_day_score.append(same_day_score)\n",
    "    \n",
    "    return bench_same_day_score, bench_diff_day_score, AE_diff_day_score, day_zero_AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222931dd",
   "metadata": {},
   "source": [
    "# Training for several days loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_over_number_of_days(start_day, epoch_N, dictListStacked, ae_learning_rt, \\\n",
    "                              convolution_filters, batch_sz, max_delta=999):\n",
    "    \n",
    "    bench_diff_day_score_mean = []\n",
    "    AE_diff_day_score_mean = []\n",
    "    bench_same_day_score_mean = []\n",
    "    bench_diff_day_score_ste = []\n",
    "    AE_diff_day_score_ste = []\n",
    "    \n",
    "    for delta in range(1, len(dictListStacked) - start_day):\n",
    "        \n",
    "        if delta > max_delta:\n",
    "            break\n",
    "        \n",
    "        train_days=[start_day, start_day + delta]\n",
    "\n",
    "        # Logger\n",
    "        logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "\n",
    "        # Train Dataset\n",
    "        signal_data = EEGDataSet_signal(dictListStacked, train_days)\n",
    "        signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "        x, y = signal_data.getAllItems()\n",
    "\n",
    "        # Train model on training day\n",
    "        day_zero_AE = convolution_AE(signal_data.n_channels, ae_learning_rt, filters_n=convolution_filters)\n",
    "        trainer_1 = pl.Trainer(max_epochs=epoch_N, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "        trainer_1.fit(day_zero_AE, train_dataloaders=signal_data_loader)\n",
    "\n",
    "        # Day 0 classifier\n",
    "        _, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N = 5, classifier = False)\n",
    "        _, day_zero_AE_clf = csp_score(np.float64(day_zero_AE(x).detach().numpy()), y, cv_N = 5, classifier = False)\n",
    "\n",
    "        # Loop :)\n",
    "        AE_diff_day_score = []\n",
    "        bench_diff_day_score = [] \n",
    "        for i in range(train_days[1], len(dictListStacked)):\n",
    "            test_days = [i, i+1]\n",
    "\n",
    "            # Create test Datasets\n",
    "            signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "            signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "            # get data\n",
    "            signal_test, y_test = signal_test_data.getAllItems()\n",
    "\n",
    "            # reconstruct EEG using day 0 AE\n",
    "            rec_signal_zero = day_zero_AE(signal_test).detach().numpy()\n",
    "\n",
    "\n",
    "            # Use models\n",
    "            bench_diff_day = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = day_zero_bench_clf)\n",
    "            AE_diff_day = csp_score(np.float64(rec_signal_zero), y_test, cv_N = 5, classifier = day_zero_AE_clf)\n",
    "            \n",
    "            # Save results\n",
    "            AE_diff_day_score.append(AE_diff_day)\n",
    "            bench_diff_day_score.append(bench_diff_day) \n",
    "        \n",
    "        # Rest of the days cross validation score\n",
    "        rest_data = EEGDataSet_signal(dictListStacked, [train_days[1], len(dictListStacked)])\n",
    "        rest_x, rest_y = rest_data.getAllItems()\n",
    "        score_bench, _= csp_score(np.float64(rest_x.detach().numpy()), rest_y, cv_N = 5, classifier = False)\n",
    "        \n",
    "        # Append means\n",
    "        bench_diff_day_score_mean.append(np.mean(bench_diff_day_score))\n",
    "        AE_diff_day_score_mean.append(np.mean(AE_diff_day_score))\n",
    "        bench_same_day_score_mean.append(score_bench)\n",
    "        bench_diff_day_score_ste.append(np.std(bench_diff_day_score) / np.sqrt(len(bench_diff_day_score)))\n",
    "        AE_diff_day_score_ste.append(np.std(AE_diff_day_score) / np.sqrt(len(AE_diff_day_score)))\n",
    "       \n",
    "    # Convert results to numpy\n",
    "    bench_same_day_score_mean = np.asarray(bench_same_day_score_mean)\n",
    "    bench_diff_day_score_mean = np.asarray(bench_diff_day_score_mean)\n",
    "    AE_diff_day_score_mean = np.asarray(AE_diff_day_score_mean)\n",
    "    bench_diff_day_score_ste = np.asarray(bench_diff_day_score_ste)\n",
    "    AE_diff_day_score_ste = np.asarray(AE_diff_day_score_ste)\n",
    "    \n",
    "    # Return results\n",
    "    return bench_same_day_score_mean, bench_diff_day_score_mean, AE_diff_day_score_mean,\\\n",
    "            bench_diff_day_score_ste, AE_diff_day_score_ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798907e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sub 206 - 200 epochs\n",
    "train_days=[0,1]\n",
    "epoch_N = 50\n",
    "\n",
    "bench_same_day_score, bench_diff_day_score, AE_diff_day_score, day_zero_AE = \\\n",
    "training_loop(train_days, epoch_N, dictListStacked, ae_learning_rt, convolution_filters, batch_sz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606f49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start from which day to plot?\n",
    "plot_from = 1\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(plot_from, plot_from + len(AE_diff_day_score[plot_from:])), AE_diff_day_score[plot_from:], label='AE diff day', color='g')\n",
    "plt.plot(range(plot_from, plot_from + len(AE_diff_day_score[plot_from:])), bench_diff_day_score[plot_from:], label='bench diff day', color='r')\n",
    "plt.plot(range(plot_from, plot_from + len(AE_diff_day_score[plot_from:])), bench_same_day_score[plot_from:], label='bench same day', color='b')\n",
    "\n",
    "plt.axhline(y=np.mean(AE_diff_day_score[plot_from:]), color='g', linestyle='--')\n",
    "plt.axhline(y=np.mean(bench_diff_day_score[plot_from:]), color='r', linestyle='--')\n",
    "plt.axhline(y=np.mean(bench_same_day_score[plot_from:]), color='b', linestyle='--')\n",
    "\n",
    "plt.title('Accuracy Over Days - Using Day 0 Classifier')\n",
    "plt.xlabel('Day #')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19e53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_score = residual_day_classifier(day_zero_AE, dictListStacked)\n",
    "rec_score = reconstruction_day_classifier(day_zero_AE, dictListStacked)\n",
    "orig_score = original_day_classifier(dictListStacked)\n",
    "\n",
    "print('Residuals Day Classify Accuracy: ', res_score)\n",
    "print('Reconstruction Day Classify Accuracy: ', rec_score)\n",
    "print('Original Day Classify Accuracy: ', orig_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = EEGDataSet_signal(dictListStacked, [3,4])\n",
    "signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "plotSignal(0, day_zero_AE, signal_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6aabe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86e20f4f",
   "metadata": {},
   "source": [
    "## Several days realizations (long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84ce96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_same_day_score_mean, bench_diff_day_score_mean, AE_diff_day_score_mean,\\\n",
    "bench_std, AE_std = \\\n",
    "score_over_number_of_days(0, 250, dictListStacked, ae_learning_rt,\\\n",
    "                          convolution_filters, batch_sz, max_delta=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(1,1+len(bench_same_day_score_mean))\n",
    "# Plots Results\n",
    "plt.plot(x_ax, AE_diff_day_score_mean, label='AE diff day', color='g')\n",
    "plt.plot(x_ax, bench_diff_day_score_mean, label='bench diff day', color='r')\n",
    "plt.plot(x_ax, bench_same_day_score_mean, label='bench same day', color='b')\n",
    "\n",
    "# # Add error area\n",
    "plt.fill_between(x_ax, bench_diff_day_score_mean-bench_std, bench_diff_day_score_mean+bench_std,\n",
    "    alpha=0.2, edgecolor='r', facecolor='r')\n",
    "plt.fill_between(x_ax, AE_diff_day_score_mean-AE_std, AE_diff_day_score_mean+AE_std,\n",
    "    alpha=0.2, edgecolor='g', facecolor='g')\n",
    "\n",
    "# Figure stuff\n",
    "plt.title('Mean Accuracy Score Over Days As Function Of Number Of Training Days')\n",
    "plt.xlabel('Number of Training Days')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bef2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddd18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7c66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd496c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a00280e1",
   "metadata": {},
   "source": [
    "# Old Junk from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_AE = []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(dictListStacked)):\n",
    "    test_days = [i, i+1]\n",
    "    \n",
    "    # Create test Datasets\n",
    "    signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "    signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "    signal_test, y_test = signal_test_data.getAllItems()\n",
    "    y.append(y_test)\n",
    "    x.append(signal_test.detach().numpy())\n",
    "    x_AE.append(day_zero_AE(signal_test).detach().numpy())\n",
    "    \n",
    "x_AE = np.concatenate(x_AE)\n",
    "x = np.concatenate(x)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_orig, _ = csp_score(np.float64(x), y, cv_N = 5, classifier = False)\n",
    "score_AE, _ = csp_score(np.float64(x_AE), y, cv_N = 5, classifier = False)\n",
    "\n",
    "print('Orig ', score_orig, '\\nAE ', score_AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cbf47",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataSet(Dataset):\n",
    "    def __init__(self, EEGDict, days_range=[0,1], test_flag = False):\n",
    "        # Concat dict      \n",
    "        signal, y = self.concat(EEGDict, days_range)\n",
    "        \n",
    "        # Features extraction\n",
    "        feat_mat = mneFeatures(signal, EEGDict[0]['fs'])\n",
    "        \n",
    "        if test_flag:\n",
    "            X = X_hat = feat_mat\n",
    "        else:     \n",
    "            all_combs, y_comb = self.get_all_combs(feat_mat, y)\n",
    "            X, X_hat = self.arrange_X(feat_mat, all_combs)\n",
    "        \n",
    "        # Convert from numpy to tensor\n",
    "        self.X = torch.tensor(X)\n",
    "        self.X_hat = torch.tensor(X_hat)\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.n_feat = self.X.shape[1]\n",
    "        self.feat_mat = feat_mat\n",
    "        self.y = y\n",
    "        if not test_flag:\n",
    "            self.y = y_comb\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float() , self.X_hat[index].float(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getAllItems(self):\n",
    "        return self.X.float() , self.X_hat.float(), self.y\n",
    "    \n",
    "    def concat(self, EEGDict, days_range):\n",
    "        X = []\n",
    "        y = []\n",
    "        for d in dictListStacked[days_range[0]:days_range[1]]:\n",
    "            X.append(d['segmentedEEG'])\n",
    "            y.append(d['labels'])\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        return X, y\n",
    "    \n",
    "    def get_all_combs(self, X, y):\n",
    "        \n",
    "        cart_product0 = product(np.argwhere(y==0).flatten(),np.argwhere(y==0).flatten())\n",
    "        cart_product1 = product(np.argwhere(y==1).flatten(),np.argwhere(y==1).flatten())\n",
    "        cart_product0 = list(cart_product0)\n",
    "        cart_product1 = list(cart_product1)\n",
    "        y_comb = np.hstack([np.zeros((1, len(cart_product0)), dtype=int), np.ones((1, len(cart_product1)),dtype=int)])\n",
    "        all_combs = cart_product0\n",
    "        all_combs.extend(cart_product1)\n",
    "        return all_combs, y_comb[0]\n",
    "    \n",
    "    def arrange_X(self, feat_mat, all_combs):\n",
    "        X = []\n",
    "        X_hat = []\n",
    "        for comb in all_combs:\n",
    "            X.append(feat_mat[comb[0],:])\n",
    "            X_hat.append(feat_mat[comb[1],:])\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        X_hat = np.asarray(X_hat)\n",
    "        return X, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9aff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lightAE(LightningModule):\n",
    "    def __init__(self, n_feat, l_comp_rate = 0.66, learning_rate=1e-3, layers_sz = False):\n",
    "        super().__init__()\n",
    "        self.n_features = n_feat\n",
    "        self.l_comp_rate = l_comp_rate\n",
    "        self.layers_sz = layers_sz\n",
    "        self.learning_rate = learning_rate\n",
    "        self.float()\n",
    "        if self.layers_sz:\n",
    "            self.l1_sz, self.l2_sz, self.l3_sz = self.layers_sz\n",
    "        else:\n",
    "            self.l1_sz = int(self.n_features*self.l_comp_rate)\n",
    "            self.l2_sz = int(self.n_features*self.l_comp_rate**2)\n",
    "            self.l3_sz = int(self.n_features*self.l_comp_rate**3)\n",
    "        \n",
    "        ### The model architecture ###\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "        nn.Linear(self.n_features, self.l1_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l1_sz, self.l2_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l2_sz, self.l3_sz),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Decoder 0\n",
    "        self.decoder_0 = nn.Sequential(\n",
    "        nn.Linear(self.l3_sz, self.l2_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l2_sz, self.l1_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l1_sz, self.n_features)\n",
    "        )\n",
    "        \n",
    "        # Decoder 1\n",
    "        self.decoder_1 = nn.Sequential(\n",
    "        nn.Linear(self.l3_sz, self.l2_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l2_sz, self.l1_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l1_sz, self.n_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, x_hat, y = batch\n",
    "        \n",
    "        latent = self.encode(x)\n",
    "        if y == 0:\n",
    "            reconstruction = self.decoder_0(latent)\n",
    "        else:\n",
    "            reconstruction = self.decoder_1(latent)\n",
    "            \n",
    "        # Loss function\n",
    "        loss = F.mse_loss(reconstruction, x_hat)\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3723e3",
   "metadata": {},
   "source": [
    "## Convolutional AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db3849",
   "metadata": {},
   "source": [
    "### Create data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc75434",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = EEGDataSet_signal(dictListStacked, train_days)\n",
    "signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "\n",
    "signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "\n",
    "features_training_data = EEGDataSet(dictListStacked, train_days, test_flag=True)\n",
    "features_data_loader = DataLoader(dataset=features_training_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "\n",
    "feautres_test_data = EEGDataSet(dictListStacked, test_days, test_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54964d20",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45da49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(max_epochs=100, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "# Create netowrk model\n",
    "conv_AE_model = convolution_AE(signal_data.n_channels, features_training_data.feat_mat, ae_learning_rt, filters_n=convolution_filters)\n",
    "# Train the model\n",
    "# trainer.fit(conv_AE_model, train_dataloaders=signal_data_loader)\n",
    "\n",
    "trainer.fit(conv_AE_model, train_dataloaders=signal_test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSignal(0, conv_AE_model, signal_test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad2100",
   "metadata": {},
   "source": [
    "## Features AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbcbbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Create logger\n",
    "# logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "# # Create trainer\n",
    "# trainer = pl.Trainer(max_epochs=n_epochs, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "# # Create netowrk model\n",
    "# features_AE_model = lightAE(training_data.n_feat, comparsion_rt, ae_learning_rt, layers_sz) #\n",
    "# # Train the model\n",
    "# trainer.fit(AEmodel, train_dataloaders=features_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4570e9d",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1156e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bench_train = features_training_data.feat_mat\n",
    "y_train = features_training_data.y\n",
    "\n",
    "X_bench_test = feautres_test_data.feat_mat\n",
    "y_test = feautres_test_data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_train, _ = signal_data.getAllItems()\n",
    "signal_test, _ = signal_test_data.getAllItems()\n",
    "\n",
    "# Use latent space as features\n",
    "X_AE_train = conv_AE_model.encode(signal_train).detach().numpy()\n",
    "X_AE_test = conv_AE_model.encode(signal_test).detach().numpy()\n",
    "\n",
    "X_AE_train = np.reshape(X_AE_train, (X_AE_train.shape[0], -1))\n",
    "X_AE_test = np.reshape(X_AE_test, (X_AE_test.shape[0], -1))\n",
    "\n",
    "# # Use recountructed signal to extract features from\n",
    "# X_AE_train = conv_AE_model(signal_train).detach().numpy()\n",
    "# X_AE_test = conv_AE_model(signal_test).detach().numpy()\n",
    "\n",
    "# X_AE_train = mneFeatures(X_AE_train, dictListStacked[0]['fs'])\n",
    "# X_AE_test = mneFeatures(X_AE_test, dictListStacked[0]['fs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compress the geatures to the same latent size as the AE\n",
    "# pca = sklearn.decomposition.PCA(n_components=X_AE_train.shape[1])\n",
    "# # PCA data sets (train_test)\n",
    "# X_pca_train = pca.fit_transform(X_bench_train)\n",
    "# X_pca_test = pca.transform(X_bench_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of datasets\n",
    "print('Bench')\n",
    "print(X_bench_train.shape)\n",
    "print(X_bench_test.shape, '\\n')\n",
    "\n",
    "print('AE')\n",
    "print(X_AE_train.shape)\n",
    "print(X_AE_test.shape, '\\n')\n",
    "\n",
    "# print('PCA')\n",
    "# print(X_pca_train.shape)\n",
    "# print(X_pca_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b672ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbModel_bench = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5)\n",
    "# lgbModel_pca = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5)\n",
    "lgbModel_ae = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5)\n",
    "\n",
    "# lgbModel_pca.fit(X_pca_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bench = cross_val_score(lgbModel_bench, X_bench_train, y_train, cv=5)\n",
    "print('Bench-\\nIn day accuracy: ', np.mean(scores_bench))\n",
    "lgbModel_bench.fit(X_bench_train, y_train)\n",
    "print('Different day accuracy: ', lgbModel_bench.score(X_bench_test, y_test), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f7955",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(lgbModel_pca.score(X_pca_test, y_test))\n",
    "scores_ae = cross_val_score(lgbModel_ae, X_AE_train, y_train, cv=5)\n",
    "print('AE-\\nIn day accuracy: ', np.mean(scores_ae))\n",
    "lgbModel_ae.fit(X_AE_train, y_train)\n",
    "print('Different day accuracy: ', lgbModel_ae.score(X_AE_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2269f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a3b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
