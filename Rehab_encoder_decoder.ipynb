{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9cdd3b",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install torch\n",
    "# !{sys.executable} -m pip install tesnorflow\n",
    "# !{sys.executable} -m pip install torchvision\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install pytorch-lightning\n",
    "# !{sys.executable} -m pip install lightning-bolts\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "# !{sys.executable} -m pip install mne-features\n",
    "# !{sys.executable} -m pip install fitter\n",
    "# !{sys.executable} -m pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d8214",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io\n",
    "import mne\n",
    "import sklearn\n",
    "import os \n",
    "import random\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain, product\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mne_features.feature_extraction import FeatureExtractor\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from scipy.stats import norm, wasserstein_distance\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b51bd",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecording(dataDir, subID, eyesCondition, day, block=[1]):\n",
    "    \"\"\"\n",
    "    Iterate over days given, of specific subject and get a list of all the files of the relevant days\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    dirPath = dataDir + '/sub' + subID + '/RA' + eyesCondition\n",
    "    for day_i in day:\n",
    "        dayStr = str(day_i)\n",
    "        if len(dayStr) == 1:\n",
    "            dayStr = '0' + dayStr\n",
    "        for block_i in block:\n",
    "            fileFormat = 'sub' + subID + '-day' + dayStr + '-block' + str(block_i) + '-condRA' + eyesCondition + '.mat'\n",
    "            data.append(scipy.io.loadmat(dirPath + '/' +fileFormat))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(eegDict):\n",
    "    \"\"\"\n",
    "    Arrange the given dictionary to more comfort dictionary\n",
    "    \"\"\"\n",
    "    # EEG will be channels_N X timestamps_N\n",
    "    EEG = eegDict['dat']['X'][0][0].T\n",
    "    # Triggers\n",
    "    triggers = np.squeeze(eegDict['dat']['Y'][0][0])\n",
    "    # Artifacts marker\n",
    "    artifacts = np.squeeze(eegDict['dat']['E'][0][0])\n",
    "    # Sampling rate \n",
    "    fs = eegDict['header']['sampleFreq'][0][0][0][0]\n",
    "    # Electrodes labels\n",
    "    chanLabels = [ch[0] for ch in eegDict['header']['Xlabels'][0][0][0]]\n",
    "    # Triggers labels\n",
    "    trigLabels = [trig[0] for trig in eegDict['header']['Ymarkers'][0][0][0]]    \n",
    "    # Trials time (in secs)\n",
    "    imagineLength = eegDict['paramRA']['c_robot'][0][0][0][0]\n",
    "    idleLength = eegDict['paramRA']['b_pause'][0][0][0][0]\n",
    "\n",
    "    Data = {'EEG': EEG, 'triggers': triggers, 'artifacts': artifacts, 'fs': fs,\n",
    "           'chanLabels': chanLabels, 'trigLabels': trigLabels, 'imagineLength': imagineLength,\n",
    "           'idleLength': idleLength}\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentEEG(eegArrangedDict, trialLen, printFlag = 1):\n",
    "    \"\"\"\n",
    "    Segment the data into epochs of MI and idle.\n",
    "    \"\"\"\n",
    "    EEG = []\n",
    "    labels = []\n",
    "    removedCount = 0\n",
    "    idleCount = 0\n",
    "    imagineCount = 0\n",
    "    \n",
    "    # Timestamps of \"move\" command\n",
    "    imgIdx = np.where(eegArrangedDict['triggers'] == 3)[0]\n",
    "    # Timestamps of 1st pause\n",
    "    idleIdx = np.where(eegArrangedDict['triggers'] == 2)[0]\n",
    "    for idx in imgIdx:\n",
    "            # Check if theres artifacts in trial (more then half the trial is labeled with artificats)          \n",
    "        if np.sum(eegArrangedDict['artifacts'][idx + 1 : idx + 1 + int(trialLen * eegArrangedDict['fs'])]) > \\\n",
    "        trialLen * eegArrangedDict['fs'] * 0.9:\n",
    "            removedCount += 1\n",
    "            # Check that the trial is atleast as the given trial length (not ended before)\n",
    "        elif np.sum(eegArrangedDict['triggers'][idx + 1 : idx + 1 + int(trialLen * eegArrangedDict['fs'])]) == 0:\n",
    "            EEG.append(eegArrangedDict['EEG'][:, idx : idx + int(trialLen * eegArrangedDict['fs'])])\n",
    "            labels.append(1)\n",
    "            imagineCount += 1\n",
    "        else:\n",
    "            removedCount += 1\n",
    "            \n",
    "    for idx in idleIdx:\n",
    "        if np.sum(eegArrangedDict['artifacts'][idx + 1 : idx + 1 + int(trialLen * eegArrangedDict['fs'])]) > 0:\n",
    "            removedCount += 1\n",
    "        else:\n",
    "            EEG.append(eegArrangedDict['EEG'][:, idx : idx + int(trialLen * eegArrangedDict['fs'])])\n",
    "            labels.append(0)\n",
    "            idleCount += 1\n",
    "    \n",
    "    # Add to the dictionary the segmented data\n",
    "    eegArrangedDict['segmentedEEG'] = np.asarray(EEG)\n",
    "    eegArrangedDict['labels'] = np.asarray(labels)\n",
    "    \n",
    "    if printFlag:\n",
    "        # Print number of trials of each class and number of removed trials\n",
    "        print(f'Imagine Trials-{imagineCount} \\nIdle Trials- {idleCount} \\nRemoved Trials- {removedCount}\\n')\n",
    "    \n",
    "    # Return the dictionary\n",
    "    return eegArrangedDict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackBlocks(eegDictList, block_N):\n",
    "    \"\"\"\n",
    "    Stack blocks from same day into one EEG + labels dictionary\n",
    "    \"\"\"\n",
    "    stackedList = []\n",
    "    count = 0\n",
    "    for i, eegDict in enumerate(eegDictList):\n",
    "        if i % block_N == 0:\n",
    "            tempArray = eegDict['segmentedEEG']\n",
    "            tempLabels = eegDict['labels']\n",
    "        else:\n",
    "            tempArray = np.concatenate((tempArray, eegDict['segmentedEEG']))\n",
    "            tempLabels = np.concatenate((tempLabels, eegDict['labels']))\n",
    "            count += 1\n",
    "        if count == block_N - 1:\n",
    "            stackedDict = {'segmentedEEG': tempArray, 'labels': tempLabels, 'fs': eegDict['fs'],\n",
    "           'chanLabels': eegDict['chanLabels'], 'trigLabels': eegDict['trigLabels'], 'trials_N': len(tempLabels)}\n",
    "            stackedList.append(stackedDict)\n",
    "            count = 0\n",
    "    \n",
    "    return stackedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57663551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eegFilters(eegMat, fs, filterLim):\n",
    "    eegMatFiltered = mne.filter.filter_data(eegMat, fs, filterLim[0], filterLim[1], verbose=0)\n",
    "    return eegMatFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMontage(chanLabels):\n",
    "    \"\"\"\n",
    "    Creates standard 10-20 location montage for given channel set\n",
    "    \"\"\"\n",
    "    montageGeneral = mne.channels.make_standard_montage('standard_1020')\n",
    "    locationDict = montageGeneral.get_positions()\n",
    "    locationDict = locationDict['ch_pos']\n",
    "    montageDict = {}\n",
    "    \n",
    "    for elec_i in chanLabels:\n",
    "        montageDict[elec_i] = locationDict[elec_i]\n",
    "\n",
    "    montage = mne.channels.make_dig_montage(montageDict)\n",
    "    return montage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mneFeatures(X, fs):\n",
    "    selected_funcs = ['line_length', 'kurtosis', 'skewness', 'pow_freq_bands', 'spect_slope',\n",
    "                     'spect_entropy', 'spect_edge_freq', 'mean', 'variance', 'ptp_amp']\n",
    "    params = {'pow_freq_bands__freq_bands' : np.array([[8, 10],\n",
    "                                                      [10,12],\n",
    "                                                      [9, 13],\n",
    "                                                      [12, 20],\n",
    "                                                      [20, 25],\n",
    "                                                      [25, 30]])}\n",
    "    fe = FeatureExtractor(sfreq=fs, selected_funcs=selected_funcs, params=params)\n",
    "\n",
    "    X_features = fe.fit_transform(X)\n",
    "    return X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadefa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2csp(signal, labels, n_components):\n",
    "    \n",
    "    # Set verbose to 0\n",
    "    mne.set_log_level(verbose='WARNING', return_old_level=False, add_frames=None)\n",
    "\n",
    "    # create csp object\n",
    "    csp = mne.decoding.CSP(n_components=n_components, reg=None, log=None, norm_trace=False, transform_into='csp_space')\n",
    "    # transofrm the signal\n",
    "    csp.fit(signal, labels)\n",
    "    csp_signal = csp.transform(signal)\n",
    "    return csp_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSignal(elec, model, dataLoader):\n",
    "    # Create iterable object\n",
    "    data_iter = iter(dataLoader)\n",
    "    data, _ = data_iter.next()\n",
    "\n",
    "    \n",
    "    # Reconstruct data using given model\n",
    "    recon_data = model(data).detach()\n",
    "    \n",
    "    \n",
    "    # Plot original and reconstructed data\n",
    "    plt.figure(1)\n",
    "    plt.plot(data[0, elec, :], zorder=1)\n",
    "    plt.plot(recon_data[0, elec, :], zorder=0)\n",
    "    plt.legend(['Original', 'Reconstructed'])\n",
    "    plt.title(f'mse-Loss: {F.mse_loss(recon_data[0, elec, :], data[0, elec, :])}')\n",
    "    plt.xlabel('Time [mS]')\n",
    "    plt.ylabel('μV')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d748b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_error(W, x):\n",
    "    x = torch.reshape(x, (x.shape[1], x.shape[0] * x.shape[2])).T\n",
    "    pc = torch.matmul(x, W)\n",
    "    x_rec = torch.matmul(pc, W.T)\n",
    "    error = torch.norm(x - x_rec)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcd17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csp_score(signal, labels, cv_N = 5, classifier = False):\n",
    "    \n",
    "    # Set verbose to 0\n",
    "    mne.set_log_level(verbose='WARNING', return_old_level=False, add_frames=None)\n",
    "\n",
    "    # Define a monte-carlo cross-validation generator (reduce variance):\n",
    "    scores = []\n",
    "    \n",
    "    if classifier:\n",
    "        y_pred = classifier.predict(signal)\n",
    "        acc = sklearn.metrics.accuracy_score(labels, y_pred)\n",
    "        return acc\n",
    "    \n",
    "    else:\n",
    "        # Assemble a classifier\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "#         lda = sklearn.ensemble.RandomForestClassifier()\n",
    "        csp = mne.decoding.CSP(n_components=16, reg=None, log=False, norm_trace=True)\n",
    "        # Use scikit-learn Pipeline with cross_val_score function\n",
    "        clf = Pipeline([('CSP', csp), ('LDA', lda)])\n",
    "        scores = cross_val_score(clf, signal, labels, cv=cv_N, n_jobs=1)\n",
    "        clf.fit(signal, labels)\n",
    "        return np.mean(scores), clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_day_classifier(eeg_list):\n",
    "    # Use day zero classifier for classifying the reconstructed eeg per day\n",
    "    \n",
    "    residuals = []\n",
    "    labels = []\n",
    "    for day_i in range(len(eeg_list)):\n",
    "        # Get data\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, [day_i, day_i+1])\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "       \n",
    "        # Get residuals\n",
    "        residuals.append(signal_test.detach().numpy())\n",
    "        labels.append(np.ones((1, signal_test.shape[0])) * day_i)\n",
    "    labels= np.hstack(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    residuals = np.vstack(residuals)\n",
    "\n",
    "    score, _ = csp_score(np.float64(residuals), labels, cv_N = 5, classifier = False)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_day_classifier(AE_model, eeg_list):\n",
    "    # Use day zero classifier for classifying the reconstructed eeg per day\n",
    "    \n",
    "    residuals = []\n",
    "    labels = []\n",
    "    for day_i in range(len(eeg_list)):\n",
    "        # Get data\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, [day_i, day_i+1])\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "\n",
    "        # reconstruct EEG AE\n",
    "        rec_signal = AE_model(signal_test).detach().numpy()        \n",
    "        # Get residuals\n",
    "        residuals.append(rec_signal)\n",
    "        labels.append(np.ones((1, signal_test.shape[0])) * day_i)\n",
    "    labels= np.hstack(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    residuals = np.vstack(residuals)\n",
    "\n",
    "    score, _ = csp_score(np.float64(residuals), labels, cv_N = 5, classifier = False)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_day_classifier(AE_model, eeg_list):\n",
    "    # Use day zero classifier for classifying the residuals per day\n",
    "    \n",
    "    residuals = []\n",
    "    labels = []\n",
    "    for day_i in range(len(eeg_list)):\n",
    "        # Get data\n",
    "        signal_test_data = EEGDataSet_signal(dictListStacked, [day_i, day_i+1])\n",
    "        signal_test, y_test = signal_test_data.getAllItems()\n",
    "\n",
    "        # reconstruct EEG AE\n",
    "        rec_signal = AE_model(signal_test).detach().numpy()        \n",
    "        # Get residuals\n",
    "        residuals.append((signal_test - rec_signal).detach().numpy())\n",
    "        labels.append(np.ones((1, signal_test.shape[0])) * day_i)\n",
    "    labels= np.hstack(labels)\n",
    "    labels = np.squeeze(labels)\n",
    "    residuals = np.vstack(residuals)\n",
    "\n",
    "    score, _ = csp_score(np.float64(residuals), labels, cv_N = 5, classifier = False)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa2af9",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "channels names:\n",
    "['FC3', 'C1', 'C3', 'C5', 'CP3', 'O1', 'FC4', 'C2', 'C4', 'C6', 'CP4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed40258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_days(dataDir, subID, eyesFlag):\n",
    "    dirPath = dataDir + '/sub' + subID + '/RA' + eyesFlag\n",
    "    onlyfiles = [f for f in os.listdir(dirPath) if f.endswith('.mat')]\n",
    "    days = []\n",
    "    for item in onlyfiles:\n",
    "        days.append(int(item.split('-')[1][3:]))\n",
    "    \n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205cc58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subID = '201' # As str 201, 205, 206\n",
    "eyesFlag = 'CC' # str        CC --> closed,   OO --> open\n",
    "dataDir = 'data'\n",
    "\n",
    "# To get all The days in subject 201:\n",
    "dayNumber = get_all_days(dataDir, subID, eyesFlag) # Array of the desired days number\n",
    "dayNumber.sort()\n",
    "# For subject 205 & 206 its better to insert range\n",
    "# dayNumber = range(2,10)\n",
    "\n",
    "# Subject 201 has only 1 block\n",
    "block = [1]\n",
    "trialLen = 6 # In seconds\n",
    "filterLim = [8, 30] # In Hz\n",
    "elec_idxs = range(11) # 0-10 according to channel names\n",
    "\n",
    "train_days = [1,2]\n",
    "test_days = [2,3]\n",
    "\n",
    "csp_feat_num = 11\n",
    "\n",
    "ae_learning_rt = 1e-3\n",
    "n_epochs = 300\n",
    "batch_sz = 16\n",
    "# If you want to use comparison rate - set layers_sz = False\n",
    "comparsion_rt = 0.8\n",
    "layers_sz = [256, 256, 256] # Length = 3\n",
    "convolution_filters = [8, 16, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c76538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relative path to absolute path\n",
    "dataDir = os.path.abspath(dataDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42c491",
   "metadata": {},
   "source": [
    "### Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all relevant days files into list\n",
    "dataList = getRecording(dataDir, subID, eyesFlag, dayNumber, block)\n",
    "\n",
    "# Extract and segment all the data\n",
    "dictList = []\n",
    "for dayData in dataList:\n",
    "    # Extract each day data\n",
    "    interData = extractData(dayData)\n",
    "    \n",
    "    # This condition is to remove some corrupted files in subject 201\n",
    "    if interData['EEG'].dtype != np.dtype('float64'):\n",
    "        continue\n",
    "        \n",
    "    # Filter the data\n",
    "    interData['EEG'] = eegFilters(interData['EEG'], interData['fs'], filterLim)\n",
    "    interData['EEG'] = interData['EEG'][elec_idxs, :]\n",
    "#     interData['EEG'] = applyICA(eegMat, eegInfo, [0], plotFlag=0)\n",
    "    # Segment the data\n",
    "    dictList.append(segmentEEG(interData, trialLen, printFlag=0))\n",
    "\n",
    "# Stack block of same day\n",
    "dictListStacked = stackBlocks(dictList, len(block))\n",
    "\n",
    "for d in dictListStacked:\n",
    "    d['csp'] = convert2csp(d['segmentedEEG'], d['labels'], csp_feat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictListStacked[0]['chanLabels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77144ecd",
   "metadata": {},
   "source": [
    "### Datset and Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataSet(Dataset):\n",
    "    def __init__(self, EEGDict, days_range=[0,1], test_flag = False):\n",
    "        # Concat dict      \n",
    "        signal, y = self.concat(EEGDict, days_range)\n",
    "        \n",
    "        # Features extraction\n",
    "        feat_mat = mneFeatures(signal, EEGDict[0]['fs'])\n",
    "        \n",
    "        if test_flag:\n",
    "            X = X_hat = feat_mat\n",
    "        else:     \n",
    "            all_combs, y_comb = self.get_all_combs(feat_mat, y)\n",
    "            X, X_hat = self.arrange_X(feat_mat, all_combs)\n",
    "        \n",
    "        # Convert from numpy to tensor\n",
    "        self.X = torch.tensor(X)\n",
    "        self.X_hat = torch.tensor(X_hat)\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.n_feat = self.X.shape[1]\n",
    "        self.feat_mat = feat_mat\n",
    "        self.y = y\n",
    "        if not test_flag:\n",
    "            self.y = y_comb\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float() , self.X_hat[index].float(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getAllItems(self):\n",
    "        return self.X.float() , self.X_hat.float(), self.y\n",
    "    \n",
    "    def concat(self, EEGDict, days_range):\n",
    "        X = []\n",
    "        y = []\n",
    "        for d in dictListStacked[days_range[0]:days_range[1]]:\n",
    "            X.append(d['segmentedEEG'])\n",
    "            y.append(d['labels'])\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        return X, y\n",
    "    \n",
    "    def get_all_combs(self, X, y):\n",
    "        \n",
    "        cart_product0 = product(np.argwhere(y==0).flatten(),np.argwhere(y==0).flatten())\n",
    "        cart_product1 = product(np.argwhere(y==1).flatten(),np.argwhere(y==1).flatten())\n",
    "        cart_product0 = list(cart_product0)\n",
    "        cart_product1 = list(cart_product1)\n",
    "        y_comb = np.hstack([np.zeros((1, len(cart_product0)), dtype=int), np.ones((1, len(cart_product1)),dtype=int)])\n",
    "        all_combs = cart_product0\n",
    "        all_combs.extend(cart_product1)\n",
    "        return all_combs, y_comb[0]\n",
    "    \n",
    "    def arrange_X(self, feat_mat, all_combs):\n",
    "        X = []\n",
    "        X_hat = []\n",
    "        for comb in all_combs:\n",
    "            X.append(feat_mat[comb[0],:])\n",
    "            X_hat.append(feat_mat[comb[1],:])\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        X_hat = np.asarray(X_hat)\n",
    "        return X, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecefb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGDataSet_signal(Dataset):\n",
    "    def __init__(self, EEGDict, days_range=[0,1]):\n",
    "        \n",
    "        # Concat dict      \n",
    "        X, y = self.concat(EEGDict, days_range)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Convert from numpy to tensor\n",
    "        self.X = torch.tensor(X)\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.n_channels = self.X.shape[1]\n",
    "        self.y = y\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].float(), self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def getAllItems(self):\n",
    "        return self.X.float() , self.y\n",
    "    \n",
    "    def concat(self, EEGDict, days_range):\n",
    "        X = []\n",
    "        y = []\n",
    "        for d in dictListStacked[days_range[0]:days_range[1]]:\n",
    "            X.append(d['segmentedEEG'])\n",
    "            y.append(d['labels'])\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_AE(LightningModule):\n",
    "    def __init__(self, input_channels, learning_rate=1e-3, filters_n = [16, 32, 64], pca_W = False):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.filters_n = filters_n\n",
    "        self.learning_rate = learning_rate\n",
    "        self.float()\n",
    "        self.l1_filters, self.l2_filters, self.l3_filters = self.filters_n\n",
    "\n",
    "        self.pca_W = pca_W\n",
    "        ### The model architecture ###\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "        nn.Conv1d(self.input_channels, self.l1_filters, kernel_size=15, stride=2, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l1_filters, self.l2_filters, kernel_size=10, stride=2, padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Conv1d(self.l2_filters, self.l3_filters, kernel_size=5, stride=2, padding=1),\n",
    "        nn.LeakyReLU()\n",
    "        )\n",
    "                \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.ConvTranspose1d(self.l3_filters, self.l2_filters, kernel_size=5, stride=2, padding=1, output_padding=1),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.ConvTranspose1d(self.l2_filters, self.l1_filters, kernel_size=10, stride=2, padding=1, output_padding=0),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.ConvTranspose1d(self.l1_filters, self.input_channels, kernel_size=15, stride=2, padding=1, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # Recountruction\n",
    "        logits = self.forward(x)\n",
    "        # Loss function\n",
    "        try:\n",
    "            if len(self.pca_W.shape)>0:\n",
    "                rec_error = pca_error(self.pca_W, logits)\n",
    "                loss = F.mse_loss(logits, x) + 0*rec_error\n",
    "        except:\n",
    "            loss = F.mse_loss(logits, x)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d26a62",
   "metadata": {},
   "source": [
    "# Huge cell for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798907e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_days=[0,30]\n",
    "\n",
    "# Shuffle the list? I like it but be aware i can create some bugs if you forget about it\n",
    "# random.shuffle(dictListStacked)\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "\n",
    "# Train Dataset\n",
    "signal_data = EEGDataSet_signal(dictListStacked, train_days)\n",
    "signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "features_training_data = EEGDataSet(dictListStacked, train_days, test_flag=True)\n",
    "\n",
    "# Compute day 0 PCA matrix\n",
    "x, y = signal_data.getAllItems()\n",
    "x_stacked = torch.reshape(x, (x.shape[1], x.shape[0] * x.shape[2])).T\n",
    "W = torch.pca_lowrank(x_stacked, x.shape[1], True)[2]\n",
    "\n",
    "# Train model on training day\n",
    "day_zero_AE = convolution_AE(signal_data.n_channels, ae_learning_rt, filters_n=convolution_filters)\n",
    "trainer_1 = pl.Trainer(max_epochs=100, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "trainer_1.fit(day_zero_AE, train_dataloaders=signal_data_loader)\n",
    "\n",
    "# Day 0 classifier\n",
    "score_ae, day_zero_AE_clf = csp_score(np.float64(day_zero_AE(x).detach().numpy()), y, cv_N = 5, classifier = False)\n",
    "score_bench, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N = 5, classifier = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a94f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop :)\n",
    "bench_diff_day_score = []\n",
    "bench_same_day_score = []\n",
    "AE_diff_day_score = []\n",
    "AE_same_day_score = []\n",
    "\n",
    "AE_list = []\n",
    "\n",
    "# Append day zero score\n",
    "bench_diff_day_score.append(score_bench)\n",
    "bench_same_day_score.append(score_bench)\n",
    "AE_diff_day_score.append(score_ae)\n",
    "AE_same_day_score.append(score_ae)\n",
    "\n",
    "for i in range(train_days[1], len(dictListStacked)):\n",
    "    test_days = [i, i+1]\n",
    "    \n",
    "    # Create test Datasets\n",
    "    signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "    signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "    \n",
    "    # Add AE model perday\n",
    "    AE_list.append(convolution_AE(signal_data.n_channels, \\\n",
    "                                                  ae_learning_rt, filters_n=convolution_filters,\\\n",
    "                                                  pca_W=False))\n",
    "\n",
    "#     # Load weights of day 0 model\n",
    "#     AE_list[i - train_days[1]].load_state_dict(day_zero_AE.state_dict())\n",
    "    \n",
    "#     # Freeze encoder layers\n",
    "#     for layer in AE_list[i - train_days[1]].encoder.parameters():\n",
    "#         layer.requires_grad=False    \n",
    "        \n",
    "#     # Run model\n",
    "#     # Create logger\n",
    "#     logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "#     # Create trainer\n",
    "#     trainer_2 = pl.Trainer(max_epochs=0, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "#     # Create netowrk model\n",
    "#     # Train the model\n",
    "#     trainer_2.fit(AE_list[i - train_days[1]], train_dataloaders=signal_test_data_loader)\n",
    "    \n",
    "    # get data\n",
    "    signal_test, y_test = signal_test_data.getAllItems()\n",
    "\n",
    "    # reconstruct EEG using same day AE\n",
    "#     rec_signal_same = AE_list[i-train_days[1]](signal_test).detach().numpy()\n",
    "    # reconstruct EEG using day 0 AE\n",
    "    rec_signal_zero = day_zero_AE(signal_test).detach().numpy()\n",
    "    \n",
    "    \n",
    "    # Use models\n",
    "    print('Day #',i)\n",
    "    same_day_score, _ = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = False)\n",
    "    print('Bench-\\nIn day accuracy: ', same_day_score)\n",
    "    bench_diff_day = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = day_zero_bench_clf)\n",
    "    print('Different day accuracy: ',bench_diff_day , '\\n')\n",
    "    \n",
    "    \n",
    "#     AE_same_day, _ = csp_score(np.float64(rec_signal_same), y_test, cv_N = 5, classifier = False)\n",
    "    print('AE-\\nSame day accuracy: ', AE_same_day)\n",
    "    AE_diff_day = csp_score(np.float64(rec_signal_zero), y_test, cv_N = 5, classifier = day_zero_AE_clf)\n",
    "    print('Different day accuracy: ', AE_diff_day)\n",
    "    \n",
    "\n",
    "    \n",
    "    AE_diff_day_score.append(AE_diff_day)\n",
    "#     AE_same_day_score.append(AE_same_day)\n",
    "    bench_diff_day_score.append(bench_diff_day) \n",
    "    bench_same_day_score.append(same_day_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606f49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(AE_diff_day_score[1:])), AE_diff_day_score[1:], label='AE diff day', color='g')\n",
    "# plt.plot(range(len(AE_diff_day_score[1:])), AE_same_day_score[1:], label='AE same day', color='y')\n",
    "plt.plot(range(len(AE_diff_day_score[1:])), bench_diff_day_score[1:], label='bench diff day', color='r')\n",
    "plt.plot(range(len(AE_diff_day_score[1:])), bench_same_day_score[1:], label='bench same day', color='b')\n",
    "\n",
    "plt.axhline(y=np.mean(AE_diff_day_score[1:]), color='g', linestyle='--')\n",
    "# plt.axhline(y=np.mean(AE_same_day_score[1:]), color='y', linestyle='--')\n",
    "plt.axhline(y=np.mean(bench_diff_day_score[1:]), color='r', linestyle='--')\n",
    "plt.axhline(y=np.mean(bench_same_day_score[1:]), color='b', linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19e53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_score = residual_day_classifier(day_zero_AE, dictListStacked)\n",
    "rec_score = reconstruction_day_classifier(day_zero_AE, dictListStacked)\n",
    "orig_score = original_day_classifier(dictListStacked)\n",
    "\n",
    "print('Residuals Day Classify Accuracy: ', res_score)\n",
    "print('Reconstruction Day Classify Accuracy: ', rec_score)\n",
    "print('Original Day Classify Accuracy: ', orig_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_over_number_of_days(start_day, max_delta=999):\n",
    "    \n",
    "    bench_diff_day_score_mean = []\n",
    "    AE_diff_day_score_mean = []\n",
    "    bench_same_day_score_mean = []\n",
    "    bench_diff_day_score_ste = []\n",
    "    AE_diff_day_score_ste = []\n",
    "    \n",
    "    for delta in range(1, len(dictListStacked) - start_day):\n",
    "        \n",
    "        if delta > max_delta:\n",
    "            break\n",
    "        \n",
    "        train_days=[start_day, start_day + delta]\n",
    "\n",
    "        # Logger\n",
    "        logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "\n",
    "        # Train Dataset\n",
    "        signal_data = EEGDataSet_signal(dictListStacked, train_days)\n",
    "        signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "\n",
    "        # Compute day 0 PCA matrix\n",
    "        x, y = signal_data.getAllItems()\n",
    "\n",
    "        # Train model on training day\n",
    "        day_zero_AE = convolution_AE(signal_data.n_channels, ae_learning_rt, filters_n=convolution_filters)\n",
    "        trainer_1 = pl.Trainer(max_epochs=10, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "        trainer_1.fit(day_zero_AE, train_dataloaders=signal_data_loader)\n",
    "\n",
    "        # Day 0 classifier\n",
    "        _, day_zero_bench_clf = csp_score(np.float64(x.detach().numpy()), y, cv_N = 5, classifier = False)\n",
    "        _, day_zero_AE_clf = csp_score(np.float64(day_zero_AE(x).detach().numpy()), y, cv_N = 5, classifier = False)\n",
    "\n",
    "        # Loop :)\n",
    "        AE_diff_day_score = []\n",
    "        bench_diff_day_score = [] \n",
    "        for i in range(train_days[1], len(dictListStacked)):\n",
    "            test_days = [i, i+1]\n",
    "\n",
    "            # Create test Datasets\n",
    "            signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "            signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "            # get data\n",
    "            signal_test, y_test = signal_test_data.getAllItems()\n",
    "\n",
    "            # reconstruct EEG using day 0 AE\n",
    "            rec_signal_zero = day_zero_AE(signal_test).detach().numpy()\n",
    "\n",
    "\n",
    "            # Use models\n",
    "            bench_diff_day = csp_score(np.float64(signal_test.detach().numpy()), y_test, cv_N = 5, classifier = day_zero_bench_clf)\n",
    "            AE_diff_day = csp_score(np.float64(rec_signal_zero), y_test, cv_N = 5, classifier = day_zero_AE_clf)\n",
    "            \n",
    "            # Save results\n",
    "            AE_diff_day_score.append(AE_diff_day)\n",
    "            bench_diff_day_score.append(bench_diff_day) \n",
    "        \n",
    "        # Rest of the days cross validation score\n",
    "        rest_data = EEGDataSet_signal(dictListStacked, [train_days[1], len(dictListStacked)])\n",
    "        rest_x, rest_y = rest_data.getAllItems()\n",
    "        score_bench, _= csp_score(np.float64(rest_x.detach().numpy()), rest_y, cv_N = 5, classifier = False)\n",
    "        \n",
    "        # Append means\n",
    "        bench_diff_day_score_mean.append(np.mean(bench_diff_day_score))\n",
    "        AE_diff_day_score_mean.append(np.mean(AE_diff_day_score))\n",
    "        bench_same_day_score_mean.append(score_bench)\n",
    "        bench_diff_day_score_ste.append(np.std(bench_diff_day_score) / np.sqrt(len(bench_diff_day_score)))\n",
    "        AE_diff_day_score_ste.append(np.std(AE_diff_day_score) / np.sqrt(len(AE_diff_day_score)))\n",
    "       \n",
    "    # Convert results to numpy\n",
    "    bench_same_day_score_mean = np.asarray(bench_same_day_score_mean)\n",
    "    bench_diff_day_score_mean = np.asarray(bench_diff_day_score_mean)\n",
    "    AE_diff_day_score_mean = np.asarray(AE_diff_day_score_mean)\n",
    "    bench_diff_day_score_ste = np.asarray(bench_diff_day_score_ste)\n",
    "    AE_diff_day_score_ste = np.asarray(AE_diff_day_score_ste)\n",
    "    \n",
    "    # Return results\n",
    "    return bench_same_day_score_mean, bench_diff_day_score_mean, AE_diff_day_score_mean,\\\n",
    "            bench_diff_day_score_ste, AE_diff_day_score_ste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84ce96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bench_same_day_score_mean, bench_diff_day_score_mean, AE_diff_day_score_mean,\\\n",
    "bench_std, AE_std = \\\n",
    "score_over_number_of_days(start_day=0, max_delta=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ax = range(1,1+len(bench_same_day_score_mean))\n",
    "# Plots Results\n",
    "plt.plot(x_ax, AE_diff_day_score_mean, label='AE diff day', color='g')\n",
    "plt.plot(x_ax, bench_diff_day_score_mean, label='bench diff day', color='r')\n",
    "plt.plot(x_ax, bench_same_day_score_mean, label='bench same day', color='b')\n",
    "\n",
    "# # Add error area\n",
    "plt.fill_between(x_ax, bench_diff_day_score_mean-bench_std, bench_diff_day_score_mean+bench_std,\n",
    "    alpha=0.2, edgecolor='r', facecolor='r')\n",
    "plt.fill_between(x_ax, AE_diff_day_score_mean-AE_std, AE_diff_day_score_mean+AE_std,\n",
    "    alpha=0.2, edgecolor='g', facecolor='g')\n",
    "\n",
    "# Figure stuff\n",
    "plt.title('Mean Accuracy Score Over Days As Function Of Number Of Training Days')\n",
    "plt.xlabel('Number of Training Days')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b260f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4138d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSignal(0, AE_list[0], signal_test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSignal(0, day_zero_AE, signal_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_AE = []\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(dictListStacked)):\n",
    "    test_days = [i, i+1]\n",
    "    \n",
    "    # Create test Datasets\n",
    "    signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "    signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "    signal_test, y_test = signal_test_data.getAllItems()\n",
    "    y.append(y_test)\n",
    "    x.append(signal_test.detach().numpy())\n",
    "    x_AE.append(day_zero_AE(signal_test).detach().numpy())\n",
    "    \n",
    "x_AE = np.concatenate(x_AE)\n",
    "x = np.concatenate(x)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_orig, _ = csp_score(np.float64(x), y, cv_N = 5, classifier = False)\n",
    "score_AE, _ = csp_score(np.float64(x_AE), y, cv_N = 5, classifier = False)\n",
    "\n",
    "print('Orig ', score_orig, '\\nAE ', score_AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cbf47",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9aff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lightAE(LightningModule):\n",
    "    def __init__(self, n_feat, l_comp_rate = 0.66, learning_rate=1e-3, layers_sz = False):\n",
    "        super().__init__()\n",
    "        self.n_features = n_feat\n",
    "        self.l_comp_rate = l_comp_rate\n",
    "        self.layers_sz = layers_sz\n",
    "        self.learning_rate = learning_rate\n",
    "        self.float()\n",
    "        if self.layers_sz:\n",
    "            self.l1_sz, self.l2_sz, self.l3_sz = self.layers_sz\n",
    "        else:\n",
    "            self.l1_sz = int(self.n_features*self.l_comp_rate)\n",
    "            self.l2_sz = int(self.n_features*self.l_comp_rate**2)\n",
    "            self.l3_sz = int(self.n_features*self.l_comp_rate**3)\n",
    "        \n",
    "        ### The model architecture ###\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "        nn.Linear(self.n_features, self.l1_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l1_sz, self.l2_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l2_sz, self.l3_sz),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Decoder 0\n",
    "        self.decoder_0 = nn.Sequential(\n",
    "        nn.Linear(self.l3_sz, self.l2_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l2_sz, self.l1_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l1_sz, self.n_features)\n",
    "        )\n",
    "        \n",
    "        # Decoder 1\n",
    "        self.decoder_1 = nn.Sequential(\n",
    "        nn.Linear(self.l3_sz, self.l2_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l2_sz, self.l1_sz),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.l1_sz, self.n_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Forward through the layeres\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, x_hat, y = batch\n",
    "        \n",
    "        latent = self.encode(x)\n",
    "        if y == 0:\n",
    "            reconstruction = self.decoder_0(latent)\n",
    "        else:\n",
    "            reconstruction = self.decoder_1(latent)\n",
    "            \n",
    "        # Loss function\n",
    "        loss = F.mse_loss(reconstruction, x_hat)\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3723e3",
   "metadata": {},
   "source": [
    "## Convolutional AE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db3849",
   "metadata": {},
   "source": [
    "### Create data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc75434",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = EEGDataSet_signal(dictListStacked, train_days)\n",
    "signal_data_loader = DataLoader(dataset=signal_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "\n",
    "signal_test_data = EEGDataSet_signal(dictListStacked, test_days)\n",
    "signal_test_data_loader = DataLoader(dataset=signal_test_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "\n",
    "features_training_data = EEGDataSet(dictListStacked, train_days, test_flag=True)\n",
    "features_data_loader = DataLoader(dataset=features_training_data, batch_size=batch_sz, shuffle=True, num_workers=0)\n",
    "\n",
    "feautres_test_data = EEGDataSet(dictListStacked, test_days, test_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54964d20",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45da49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(max_epochs=100, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "# Create netowrk model\n",
    "conv_AE_model = convolution_AE(signal_data.n_channels, features_training_data.feat_mat, ae_learning_rt, filters_n=convolution_filters)\n",
    "# Train the model\n",
    "# trainer.fit(conv_AE_model, train_dataloaders=signal_data_loader)\n",
    "\n",
    "trainer.fit(conv_AE_model, train_dataloaders=signal_test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d3478",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSignal(0, conv_AE_model, signal_test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad2100",
   "metadata": {},
   "source": [
    "## Features AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbcbbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Create logger\n",
    "# logger = TensorBoardLogger('tb_logs', name='EEG_Logger')\n",
    "# # Create trainer\n",
    "# trainer = pl.Trainer(max_epochs=n_epochs, gpus=0, progress_bar_refresh_rate=20, logger=logger)\n",
    "# # Create netowrk model\n",
    "# features_AE_model = lightAE(training_data.n_feat, comparsion_rt, ae_learning_rt, layers_sz) #\n",
    "# # Train the model\n",
    "# trainer.fit(AEmodel, train_dataloaders=features_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4570e9d",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1156e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bench_train = features_training_data.feat_mat\n",
    "y_train = features_training_data.y\n",
    "\n",
    "X_bench_test = feautres_test_data.feat_mat\n",
    "y_test = feautres_test_data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d1941",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_train, _ = signal_data.getAllItems()\n",
    "signal_test, _ = signal_test_data.getAllItems()\n",
    "\n",
    "# Use latent space as features\n",
    "X_AE_train = conv_AE_model.encode(signal_train).detach().numpy()\n",
    "X_AE_test = conv_AE_model.encode(signal_test).detach().numpy()\n",
    "\n",
    "X_AE_train = np.reshape(X_AE_train, (X_AE_train.shape[0], -1))\n",
    "X_AE_test = np.reshape(X_AE_test, (X_AE_test.shape[0], -1))\n",
    "\n",
    "# # Use recountructed signal to extract features from\n",
    "# X_AE_train = conv_AE_model(signal_train).detach().numpy()\n",
    "# X_AE_test = conv_AE_model(signal_test).detach().numpy()\n",
    "\n",
    "# X_AE_train = mneFeatures(X_AE_train, dictListStacked[0]['fs'])\n",
    "# X_AE_test = mneFeatures(X_AE_test, dictListStacked[0]['fs'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compress the geatures to the same latent size as the AE\n",
    "# pca = sklearn.decomposition.PCA(n_components=X_AE_train.shape[1])\n",
    "# # PCA data sets (train_test)\n",
    "# X_pca_train = pca.fit_transform(X_bench_train)\n",
    "# X_pca_test = pca.transform(X_bench_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of datasets\n",
    "print('Bench')\n",
    "print(X_bench_train.shape)\n",
    "print(X_bench_test.shape, '\\n')\n",
    "\n",
    "print('AE')\n",
    "print(X_AE_train.shape)\n",
    "print(X_AE_test.shape, '\\n')\n",
    "\n",
    "# print('PCA')\n",
    "# print(X_pca_train.shape)\n",
    "# print(X_pca_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b672ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbModel_bench = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5)\n",
    "# lgbModel_pca = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5)\n",
    "lgbModel_ae = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5)\n",
    "\n",
    "# lgbModel_pca.fit(X_pca_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bench = cross_val_score(lgbModel_bench, X_bench_train, y_train, cv=5)\n",
    "print('Bench-\\nIn day accuracy: ', np.mean(scores_bench))\n",
    "lgbModel_bench.fit(X_bench_train, y_train)\n",
    "print('Different day accuracy: ', lgbModel_bench.score(X_bench_test, y_test), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f7955",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(lgbModel_pca.score(X_pca_test, y_test))\n",
    "scores_ae = cross_val_score(lgbModel_ae, X_AE_train, y_train, cv=5)\n",
    "print('AE-\\nIn day accuracy: ', np.mean(scores_ae))\n",
    "lgbModel_ae.fit(X_AE_train, y_train)\n",
    "print('Different day accuracy: ', lgbModel_ae.score(X_AE_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102e577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2269f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a3b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
